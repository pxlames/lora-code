# 微调与Agentic RL技术学习心得

## 已掌握内容

我已经系统学习了大语言模型的微调技术，主要包括以下内容：

- **SFT（Supervised Fine-Tuning）**：有监督微调，掌握了基本流程和数据准备方法。
- **PEFT（Parameter-Efficient Fine-Tuning）**：参数高效微调，重点学习了以下方法：
  - **LoRA（Low-Rank Adaptation）**：了解了其原理和在大模型微调中的应用。
  - **QLoRA**：掌握了量化与低秩结合的高效微调方式。
  - **peft库**：熟悉了 peft 库的使用方法，能够灵活调用其各类微调方案。
- **Xtuner**：已经接触并初步使用过 Xtuner 工具，了解其在微调流程中的作用。

## RL 框架梳理

在支持 Agentic RL 研究的开源框架方面，已梳理如下：

- **专用 Agentic RL 框架**：如 SkyRL、AREAL，专注于智能体强化学习的研究与实现。
- **通用 RLHF 框架**：如 TRL、OpenRLHF，适用于人类反馈强化学习的主流流程。
- **底层通用 RL 库**：如 RLlib，提供灵活的底层 RL 算法支持。

## Agentic RL 方法学习体会

- **RL 之前的主流方法**：主要依赖 SFT（如 Toolformer 在专家轨迹上微调）或提示工程（如 ReAct）。这些方法本质上属于模仿学习，智能体只能复现见过的工具使用模式，缺乏策略灵活性和错误恢复能力。
- **RL 驱动的工具使用**：强化学习将目标从“模仿”转向“结果驱动的优化”。智能体不再只是简单模仿工具调用，而是学习何时、为何以及如何组合使用工具以最大化任务成功率。ToRL、ToolRL 等工作表明，即使从零开始，RL 也能让智能体涌现出自主纠错、工具组合等复杂行为。
- **未来展望：长时程工具集成推理（Long-horizon TIR）**：当前的主要瓶颈在于时间信用分配（temporal credit assignment）。在包含数十步工具调用的长任务中，如何判断哪一步是关键的成功或失败因素，是亟待解决的难题。

## 后续学习计划

接下来，我准备进一步学习和掌握以下内容：

- **Unsloth**：探索更高效的微调实现方案。
- **llama_factory**：低代码平台。
- **Deepspeed 分布式训练框架**：学习大规模分布式训练与推理的实现方法，提升训练效率和模型规模。
- **RLHF（Reinforcement Learning from Human Feedback）**：系统学习其原理与实现流程，主要包括以下模块：
  - **奖励模型（Reward Model）**：学习如何构建和训练奖励模型。
  - **PPO（Proximal Policy Optimization）算法**：掌握主流的RLHF优化算法。
  - **数据收集与标注**：了解高质量人类反馈数据的采集与处理方法。
  - **RLHF训练流程实操**：从SFT到奖励建模再到PPO微调的完整流程。
  - **主流RLHF开源工具**：如trl、DeepSpeed-Chat等的使用方法。
  - **RLHF在大模型中的应用案例**：分析典型RLHF落地项目。

## 目标

通过持续学习和实践，进一步提升大模型微调与部署能力，掌握主流和前沿的高效微调及Agentic RL技术，能够独立完成大模型的定制化训练与优化，未来能够熟练掌握RLHF及Agentic RL相关技术并应用于实际项目。
